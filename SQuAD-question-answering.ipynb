{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2940905c-48ec-4810-8e19-7b7ac33e2bda",
   "metadata": {},
   "source": [
    "# SQuAD Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b0eef-83bc-4ba5-ab39-7a73c11bacbc",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab54ce-bf7a-4824-a4da-5b03d30ace70",
   "metadata": {},
   "source": [
    "Welcome to the \"Enhancing Question Answering with Transformer Models\" project! In this endeavor, I will be delving into the realm of Natural Language Processing to tackle the challenging task of building a model that can accurately comprehend and answer questions based on given contexts. By harnessing the transformative capabilities of Transformer architectures, I aim to create a robust system that not only understands the nuances of human language but also delivers contextually relevant answers.\n",
    "\n",
    "The heart of our project beats with the transformative potential of Transformer architecture, a groundbreaking innovation that has revolutionized the field of NLP. Inspired by the \"Attention is All You Need\" paper, we will harness the capabilities of self-attention mechanisms, multi-head attention, and feedforward neural networks to build models that can efficiently capture intricate linguistic relationships, even in lengthy and complex text.\n",
    "\n",
    "As I progress through this project, I'll explore data preprocessing, model selection, fine-tuning, and evaluation methodologies, aiming to equip the model with the ability to interpret context and contextually generate insightful answers. Whether it's tackling questions on passages of text, summarizing content, or generating human-like responses, this project is a tribute to the power of modern AI in understanding and manipulating language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f3fe9-aaf5-4333-897b-2081cca7124d",
   "metadata": {},
   "source": [
    "## 2 Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc1d32-d32c-4f31-ae8e-a84b6841ebe2",
   "metadata": {},
   "source": [
    "### 2.1 Packages Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93074cd-3727-4e2a-9432-fca4eb79339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.1\n",
      "torchvision version: 0.15.2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from transformers import pipeline\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    !pip install torch torchvision\n",
    "    !pip install transformers\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from transformers import pipeline\n",
    "    print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e904b2a-fcc1-4f45-a1cb-83ea99ed55ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b7c19-b58f-415c-83e0-d01408b8b6a0",
   "metadata": {},
   "source": [
    "### 2.2 Acquiring The SQuAD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531194cb-834e-41f6-acf0-451852090d12",
   "metadata": {},
   "source": [
    "## 3 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8c6d2-23c8-4e7c-914a-7a617661bba9",
   "metadata": {},
   "source": [
    "## 4 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957b093-0623-413e-848c-23be48623465",
   "metadata": {},
   "source": [
    "## 5 Designing a Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1616da8-c2ce-43e3-ac94-d8def8719738",
   "metadata": {},
   "source": [
    "### 5.1 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67435db5-b627-46b0-abb7-21a55daabf7a",
   "metadata": {},
   "source": [
    "### 5.2 Model Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aaabbf-efdf-41fa-97ed-718ccf740991",
   "metadata": {},
   "source": [
    "### 5.3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b649d1-48a8-46e8-af66-602c0a0a5d27",
   "metadata": {},
   "source": [
    "### 5.4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb71ed-53a2-41eb-935e-bd982804621f",
   "metadata": {},
   "source": [
    "## 6 Implementing Transformer-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374831b6-1776-4fe4-8b9e-82e9025358b1",
   "metadata": {},
   "source": [
    "## 7 Using the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d36a5-9708-400c-8c16-545ae6550877",
   "metadata": {},
   "source": [
    "## 8 Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
