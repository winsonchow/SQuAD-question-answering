{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2940905c-48ec-4810-8e19-7b7ac33e2bda",
   "metadata": {},
   "source": [
    "# SQuAD Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b0eef-83bc-4ba5-ab39-7a73c11bacbc",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab54ce-bf7a-4824-a4da-5b03d30ace70",
   "metadata": {},
   "source": [
    "Welcome to the \"Enhancing Question Answering with Transformer Models\" project! In this endeavor, I will be delving into the realm of Natural Language Processing to tackle the challenging task of building a model that can accurately comprehend and answer questions based on given contexts. By harnessing the transformative capabilities of Transformer architectures, I aim to create a robust system that not only understands the nuances of human language but also delivers contextually relevant answers.\n",
    "\n",
    "The heart of our project beats with the transformative potential of Transformer architecture, a groundbreaking innovation that has revolutionized the field of NLP. Inspired by the \"Attention is All You Need\" paper, we will harness the capabilities of self-attention mechanisms, multi-head attention, and feedforward neural networks to build models that can efficiently capture intricate linguistic relationships, even in lengthy and complex text.\n",
    "\n",
    "As I progress through this project, I'll explore data preprocessing, model selection, fine-tuning, and evaluation methodologies, aiming to equip the model with the ability to interpret context and contextually generate insightful answers. Whether it's tackling questions on passages of text, summarizing content, or generating human-like responses, this project is a tribute to the power of modern AI in understanding and manipulating language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f3fe9-aaf5-4333-897b-2081cca7124d",
   "metadata": {},
   "source": [
    "## 2 Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc1d32-d32c-4f31-ae8e-a84b6841ebe2",
   "metadata": {},
   "source": [
    "### 2.1 Packages Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93074cd-3727-4e2a-9432-fca4eb79339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.1\n",
      "torchvision version: 0.15.2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from transformers import pipeline\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    !pip install torch torchvision\n",
    "    !pip install transformers\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from transformers import pipeline\n",
    "    print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e904b2a-fcc1-4f45-a1cb-83ea99ed55ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b7c19-b58f-415c-83e0-d01408b8b6a0",
   "metadata": {},
   "source": [
    "### 2.2 Acquiring The SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf2793c-bf30-4ae6-a80c-57c9cd05d02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/66/f8/38298237d18d4b6a8ee5dfe390e97bed5adb8e01ec6f9680c0ddf3066728/datasets-2.14.4-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./project-env/lib/python3.10/site-packages (from datasets) (1.25.2)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=8.0.0 from https://files.pythonhosted.org/packages/77/0d/3a698f5fee20e6086017ae8a0fe8eac40eebceb7dc66e96993b10503ad58/pyarrow-13.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pyarrow-13.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/4a/f6/f620ca62365d83e663a255a41b08d2fc2eaf304e0b8b21bb6d62a7390fe3/pandas-2.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pandas-2.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./project-env/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./project-env/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/5f/0f/0571935f4869850bf36728c778b9975260c28ac678f6d23a3b6944449da6/xxhash-3.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/35/a8/36d8d7b3e46b377800d8dec47891cdf05842d1a2366909ae4a0c89fbc5e6/multiprocess-0.70.15-py310-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./project-env/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/fa/9e/49002fde2a97d7df0e162e919c31cf13aa9f184537739743d1239edd0e67/aiohttp-3.8.5-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in ./project-env/lib/python3.10/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in ./project-env/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./project-env/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./project-env/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./project-env/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.2-cp310-cp310-macosx_11_0_arm64.whl (62 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/67/6a/55a49da0fa373ac9aa49ccd5b6393ecc183e2a0904d9449ea3ee1163e0b1/frozenlist-1.4.0-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in ./project-env/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./project-env/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./project-env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./project-env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./project-env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./project-env/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./project-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.14.4-py3-none-any.whl (519 kB)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached aiohttp-3.8.5-cp310-cp310-macosx_11_0_arm64.whl (343 kB)\n",
      "Downloading pyarrow-13.0.0-cp310-cp310-macosx_11_0_arm64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "Downloading pandas-2.0.3-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached xxhash-3.3.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.0-cp310-cp310-macosx_11_0_arm64.whl (46 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.4 dill-0.3.7 frozenlist-1.4.0 multidict-6.0.4 multiprocess-0.70.15 pandas-2.0.3 pyarrow-13.0.0 pytz-2023.3 tzdata-2023.3 xxhash-3.3.0 yarl-1.9.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fe091fba7042e0ac689495a32b67e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f0f8922939475da22caa6cb5379c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a30a6a1ad64661bd800d677d522418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0e167ca5b14732b56d4a9ac77bf3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72559daf781840dbb4e967d4f3e0f80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f743e7a2eac34d2694e727d9ce6afa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cda91fed9544413aa385e1ec05299a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae593a3c1854833b3fa4259725e2a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659d466772ac48ceb75afa00300ef08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    from datasets import load_dataset\n",
    "    raw_datasets = load_dataset(\"squad\")\n",
    "except:\n",
    "    !pip install datasets\n",
    "    from datasets import load_dataset\n",
    "    raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e1daae-2915-481d-b9fd-901bcdffc562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531194cb-834e-41f6-acf0-451852090d12",
   "metadata": {},
   "source": [
    "## 3 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9ae29-0991-47b5-a8ec-83a9f6d8b0d2",
   "metadata": {},
   "source": [
    "Data exploration is a crucial step that allows us to understand the nuances and characteristics of the dataset, enabling us to make informed decisions during preprocessing and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62815b-5d5b-40a2-bb2e-ba85a93c1e39",
   "metadata": {},
   "source": [
    "### 3.1 Basic Statistics\n",
    "To perform data exploration on the SQuAD dataset, we're following a systematic process. We're starting by calculating basic statistics such as the number of examples in the dataset, the average length of questions and contexts, and the distribution of answer lengths. These statistics give us a high-level overview of the dataset's composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c43e0151-e626-4015-a9a5-204f4e9e2b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 87599\n",
      "Average context length: 754.36\n",
      "Average question length: 59.57\n",
      "Average answer length: 20.15\n"
     ]
    }
   ],
   "source": [
    "# Access the training split\n",
    "train_data = raw_datasets[\"train\"]\n",
    "\n",
    "# Basic statistics\n",
    "num_examples = len(train_data)\n",
    "context_lengths = [len(example[\"context\"]) for example in train_data]\n",
    "question_lengths = [len(example[\"question\"]) for example in train_data]\n",
    "answer_lengths = [len(example[\"answers\"][\"text\"][0]) for example in train_data]\n",
    "\n",
    "avg_context_length = sum(context_lengths) / num_examples\n",
    "avg_question_length = sum(question_lengths) / num_examples\n",
    "avg_answer_length = sum(answer_lengths) / num_examples\n",
    "\n",
    "print(f\"Number of examples: {num_examples}\")\n",
    "print(f\"Average context length: {avg_context_length:.2f}\")\n",
    "print(f\"Average question length: {avg_question_length:.2f}\")\n",
    "print(f\"Average answer length: {avg_answer_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb8c5b-8fed-495c-91c6-242a16cca586",
   "metadata": {},
   "source": [
    "### 3.2 Sample Viewing\n",
    "Next, we're randomly sampling examples from the dataset and visually inspecting them. This hands-on approach helps us grasp the format of questions, contexts, and answer spans. We're also using visualizations such as histograms and box plots to analyze the distribution of question and context lengths, aiding in identifying potential outliers or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583a045c-0068-416f-a9f1-7e96ea8fb868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. \n",
      "\n",
      "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? \n",
      "\n",
      "Answer:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Context: \", train_data[0][\"context\"], \"\\n\")\n",
    "print(\"Question: \", train_data[0][\"question\"], \"\\n\")\n",
    "print(\"Answer: \", train_data[0][\"answers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97a562-c5dd-41a3-ae82-64db4fff1375",
   "metadata": {},
   "source": [
    "### 3.3 Answer Types and Categories\n",
    "Finally, we will be exploring the different types of answers present in the dataset (e.g., named entities, numeric answers, descriptive answers). This information can guide our preprocessing and model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957634bb-159b-42e0-abbd-cbf240b312ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Answers: 6912\n",
      "Named Entities: 1225\n",
      "Descriptive Answers: 56857\n",
      "Other Answers: 22605\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regular expression patterns to identify answer types\n",
    "numeric_pattern = re.compile(r'^\\d+(\\.\\d+)?$')\n",
    "\n",
    "# Initialize counters for different answer types\n",
    "numeric_answers = 0\n",
    "named_entities = 0\n",
    "descriptive_answers = 0\n",
    "other_answers = 0\n",
    "\n",
    "# Loop through examples to categorize answers\n",
    "for example in train_data:\n",
    "    answer_text = example[\"answers\"][\"text\"][0]\n",
    "    \n",
    "    if re.match(numeric_pattern, answer_text):\n",
    "        numeric_answers += 1\n",
    "    elif answer_text.isupper():\n",
    "        named_entities += 1\n",
    "    elif len(answer_text.split()) > 1:\n",
    "        descriptive_answers += 1\n",
    "    else:\n",
    "        other_answers += 1\n",
    "\n",
    "# Print the counts for different answer types\n",
    "print(f\"Numeric Answers: {numeric_answers}\")\n",
    "print(f\"Named Entities: {named_entities}\")\n",
    "print(f\"Descriptive Answers: {descriptive_answers}\")\n",
    "print(f\"Other Answers: {other_answers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8c6d2-23c8-4e7c-914a-7a617661bba9",
   "metadata": {},
   "source": [
    "## 4 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957b093-0623-413e-848c-23be48623465",
   "metadata": {},
   "source": [
    "## 5 Designing a Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1616da8-c2ce-43e3-ac94-d8def8719738",
   "metadata": {},
   "source": [
    "### 5.1 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67435db5-b627-46b0-abb7-21a55daabf7a",
   "metadata": {},
   "source": [
    "### 5.2 Model Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aaabbf-efdf-41fa-97ed-718ccf740991",
   "metadata": {},
   "source": [
    "### 5.3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b649d1-48a8-46e8-af66-602c0a0a5d27",
   "metadata": {},
   "source": [
    "### 5.4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb71ed-53a2-41eb-935e-bd982804621f",
   "metadata": {},
   "source": [
    "## 6 Implementing Transformer-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374831b6-1776-4fe4-8b9e-82e9025358b1",
   "metadata": {},
   "source": [
    "## 7 Using the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d36a5-9708-400c-8c16-545ae6550877",
   "metadata": {},
   "source": [
    "## 8 Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
