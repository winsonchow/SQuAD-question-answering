{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2940905c-48ec-4810-8e19-7b7ac33e2bda",
   "metadata": {},
   "source": [
    "# SQuAD Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b0eef-83bc-4ba5-ab39-7a73c11bacbc",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab54ce-bf7a-4824-a4da-5b03d30ace70",
   "metadata": {},
   "source": [
    "Welcome to the \"Enhancing Question Answering with Transformer Models\" project! In this endeavor, I will be delving into the realm of Natural Language Processing to tackle the challenging task of building a model that can accurately comprehend and answer questions based on given contexts. By harnessing the transformative capabilities of Transformer architectures, I aim to create a robust system that not only understands the nuances of human language but also delivers contextually relevant answers.\n",
    "\n",
    "The heart of our project beats with the transformative potential of Transformer architecture, a groundbreaking innovation that has revolutionized the field of NLP. Inspired by the \"Attention is All You Need\" paper, we will harness the capabilities of self-attention mechanisms, multi-head attention, and feedforward neural networks to build models that can efficiently capture intricate linguistic relationships, even in lengthy and complex text.\n",
    "\n",
    "As I progress through this project, I'll explore data preprocessing, model selection, fine-tuning, and evaluation methodologies, aiming to equip the model with the ability to interpret context and contextually generate insightful answers. Whether it's tackling questions on passages of text, summarizing content, or generating human-like responses, this project is a tribute to the power of modern AI in understanding and manipulating language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f3fe9-aaf5-4333-897b-2081cca7124d",
   "metadata": {},
   "source": [
    "## 2 Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc1d32-d32c-4f31-ae8e-a84b6841ebe2",
   "metadata": {},
   "source": [
    "### 2.1 Packages Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93074cd-3727-4e2a-9432-fca4eb79339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.1\n",
      "torchvision version: 0.15.2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from transformers import pipeline\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    !pip install torch torchvision\n",
    "    !pip install transformers\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from transformers import pipeline\n",
    "    print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e904b2a-fcc1-4f45-a1cb-83ea99ed55ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b7c19-b58f-415c-83e0-d01408b8b6a0",
   "metadata": {},
   "source": [
    "### 2.2 Acquiring The SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf2793c-bf30-4ae6-a80c-57c9cd05d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from datasets import load_dataset\n",
    "    raw_datasets = load_dataset(\"squad\")\n",
    "except:\n",
    "    !pip install datasets\n",
    "    from datasets import load_dataset\n",
    "    raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e1daae-2915-481d-b9fd-901bcdffc562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531194cb-834e-41f6-acf0-451852090d12",
   "metadata": {},
   "source": [
    "## 3 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9ae29-0991-47b5-a8ec-83a9f6d8b0d2",
   "metadata": {},
   "source": [
    "Data exploration is a crucial step that allows us to understand the nuances and characteristics of the dataset, enabling us to make informed decisions during preprocessing and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62815b-5d5b-40a2-bb2e-ba85a93c1e39",
   "metadata": {},
   "source": [
    "### 3.1 Basic Statistics\n",
    "To perform data exploration on the SQuAD dataset, we're following a systematic process. We're starting by calculating basic statistics such as the number of examples in the dataset, the average length of questions and contexts, and the distribution of answer lengths. These statistics give us a high-level overview of the dataset's composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c43e0151-e626-4015-a9a5-204f4e9e2b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 87599\n",
      "Average context length: 754.36\n",
      "Average question length: 59.57\n",
      "Average answer length: 20.15\n"
     ]
    }
   ],
   "source": [
    "# Access the training split\n",
    "train_data = raw_datasets[\"train\"]\n",
    "\n",
    "# Basic statistics\n",
    "num_examples = len(train_data)\n",
    "context_lengths = [len(example[\"context\"]) for example in train_data]\n",
    "question_lengths = [len(example[\"question\"]) for example in train_data]\n",
    "answer_lengths = [len(example[\"answers\"][\"text\"][0]) for example in train_data]\n",
    "\n",
    "avg_context_length = sum(context_lengths) / num_examples\n",
    "avg_question_length = sum(question_lengths) / num_examples\n",
    "avg_answer_length = sum(answer_lengths) / num_examples\n",
    "\n",
    "print(f\"Number of examples: {num_examples}\")\n",
    "print(f\"Average context length: {avg_context_length:.2f}\")\n",
    "print(f\"Average question length: {avg_question_length:.2f}\")\n",
    "print(f\"Average answer length: {avg_answer_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb8c5b-8fed-495c-91c6-242a16cca586",
   "metadata": {},
   "source": [
    "### 3.2 Sample Viewing\n",
    "Next, we're randomly sampling examples from the dataset and visually inspecting them. This hands-on approach helps us grasp the format of questions, contexts, and answer spans. We're also using visualizations such as histograms and box plots to analyze the distribution of question and context lengths, aiding in identifying potential outliers or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583a045c-0068-416f-a9f1-7e96ea8fb868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. \n",
      "\n",
      "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? \n",
      "\n",
      "Answer:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Context: \", train_data[0][\"context\"], \"\\n\")\n",
    "print(\"Question: \", train_data[0][\"question\"], \"\\n\")\n",
    "print(\"Answer: \", train_data[0][\"answers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97a562-c5dd-41a3-ae82-64db4fff1375",
   "metadata": {},
   "source": [
    "### 3.3 Answer Types and Categories\n",
    "Finally, we will be exploring the different types of answers present in the dataset (e.g., named entities, numeric answers, descriptive answers). This information can guide our preprocessing and model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957634bb-159b-42e0-abbd-cbf240b312ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Answers: 6912\n",
      "Named Entities: 1225\n",
      "Descriptive Answers: 56857\n",
      "Other Answers: 22605\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regular expression patterns to identify answer types\n",
    "numeric_pattern = re.compile(r'^\\d+(\\.\\d+)?$')\n",
    "\n",
    "# Initialize counters for different answer types\n",
    "numeric_answers = 0\n",
    "named_entities = 0\n",
    "descriptive_answers = 0\n",
    "other_answers = 0\n",
    "\n",
    "# Loop through examples to categorize answers\n",
    "for example in train_data:\n",
    "    answer_text = example[\"answers\"][\"text\"][0]\n",
    "    \n",
    "    if re.match(numeric_pattern, answer_text):\n",
    "        numeric_answers += 1\n",
    "    elif answer_text.isupper():\n",
    "        named_entities += 1\n",
    "    elif len(answer_text.split()) > 1:\n",
    "        descriptive_answers += 1\n",
    "    else:\n",
    "        other_answers += 1\n",
    "\n",
    "# Print the counts for different answer types\n",
    "print(f\"Numeric Answers: {numeric_answers}\")\n",
    "print(f\"Named Entities: {named_entities}\")\n",
    "print(f\"Descriptive Answers: {descriptive_answers}\")\n",
    "print(f\"Other Answers: {other_answers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413098b1-8532-4665-abbf-d3e39720d017",
   "metadata": {},
   "source": [
    "### 3.4 Dataset Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df37349-8a03-4e8b-9ae4-0062d22689b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8c6d2-23c8-4e7c-914a-7a617661bba9",
   "metadata": {},
   "source": [
    "## 4 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f84b693-6822-46a6-b18d-b95117fa401e",
   "metadata": {},
   "source": [
    "Data preprocessing plays a pivotal role in our project focused on enhancing question answering using Transformer-based models. It serves as the foundation that empowers these advanced models to understand and interpret human language effectively. By transforming raw text into a structured format, we enable the models to process, learn from, and generate accurate responses based on the input.\n",
    "\n",
    "Transformers operate at the token level, and each token typically corresponds to a word or subword unit. Preprocessing involves tokenizing the text into these units, allowing the model to understand and process the input. Tokenization is a fundamental step in preparing text data for Transformer models.\n",
    "\n",
    "Fortunately, we can import AutoTokenizer from the transformers library. AutoTokenizer covers:\n",
    "1. Tokenization\n",
    "2. Padding and Truncation\n",
    "3. Adding Special Tokens\n",
    "4. Positional Encodings\n",
    "5. Encoding and Decoding\n",
    "6. Batching\n",
    "7. Mapping to Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb1f87-c3c5-43bf-9163-a732935dad09",
   "metadata": {},
   "source": [
    "### 4.1 Importing the AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b48bc402-933f-4f2c-823c-06637534b4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sample text for tokenization\n",
    "context = train_data[0][\"context\"]\n",
    "question = train_data[0][\"question\"]\n",
    "tokenized_data = tokenizer(context, question)\n",
    "\n",
    "# Print the first tokenized example\n",
    "tokenizer.decode(tokenized_data[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad29f6fa-df48-4471-a01b-6cf1a20c1381",
   "metadata": {},
   "source": [
    "### 4.2 Exploring the Tokenizer Properties\n",
    "Modifying tokenizer properties is a crucial step in preparing the SQuAD dataset for training with Transformer-based models. Tokenization is not a one-size-fits-all process; it needs to be customized to suit the unique characteristics of the dataset and task. By modifying tokenizer properties, we ensure that the input data is processed in a way that maximizes the efficiency and effectiveness of the subsequent training process.\n",
    "\n",
    "1. **Sequence Length Management:**\n",
    "   By setting the `max_length` parameter to 100, we are restricting the maximum length of the tokenized sequences. This ensures that the tokenized sequences fit within the model's input limitations. Modifying `max_length` allows us to control the input size while considering the tokenization requirements of the model.\n",
    "\n",
    "2. **Padding and Overflowing Tokens:**\n",
    "   The `return_overflowing_tokens=True` parameter allows the tokenizer to return overflowing tokens when the input exceeds the specified `max_length`. Modifying this parameter helps you capture complete context information even if it exceeds the set length. This is particularly important for tasks like question answering, where the context is critical for generating accurate answers.\n",
    "\n",
    "3. **Stride for Overlapping Contexts:**\n",
    "   The `stride` parameter specifies the step size when sliding the tokenized window over the context. This can lead to overlapping context windows, which can be beneficial for maintaining context continuity across different tokenized examples. This approach ensures that information is not lost when splitting long contexts into smaller segments.\n",
    "\n",
    "4. **Truncation Strategy:**\n",
    "   We're using `truncation=\"only_second\"` which truncates the `context` portion of the input in case it exceeds the `max_length`. This strategy maintains the complete question and prioritizes retaining the latter portion of the context, which often contains more relevant information.\n",
    "\n",
    "5. **Decoding and Understanding Tokens:**\n",
    "   The loop that decodes and prints the `input_ids` provides insight into how the tokenized sequences are represented. This helps you understand how tokenization affects the structure of the input and provides a way to verify the preprocessing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bcde4c0-c512-4b14-aeb5-681ebc93a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the question and context using the tokenizer\n",
    "inputs = tokenizer(\n",
    "    question,                                 # The question to be tokenized\n",
    "    context,                                  # The context to be tokenized\n",
    "    max_length=100,                           # Set the maximum combined sequence length\n",
    "    truncation=\"only_second\",                 # Truncate context if it exceeds max_length\n",
    "    stride=50,                                # Step size for sliding the tokenized window\n",
    "    return_overflowing_tokens=True,           # Return overflowing tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2805f0e4-e474-41aa-a74c-7b0c9a259889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the gr [SEP] \n",
      "\n",
      "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint [SEP] \n",
      "\n",
      "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome [SEP] \n",
      "\n",
      "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the input_ids of the tokenized sequences\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    # Decode and print the tokenized sequence\n",
    "    print(tokenizer.decode(ids), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5df5fee-0b8e-4e96-8422-5a1763cc3536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n"
     ]
    }
   ],
   "source": [
    "# Print the keys in the 'inputs' dictionary\n",
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835d12d-df17-4a1a-a045-b4a8e4edaa11",
   "metadata": {},
   "source": [
    "### 4.3 Tokenizing More Examples\n",
    "This code essentially demonstrates how the tokenizer processes a subset of the dataset, tokenizing questions and contexts and providing additional information about the generated features and their mapping to the original examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9d53b9-37d5-4fc1-aabc-26dcaca01b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 17 examples gave 17 features.\n",
      "Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3].\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the questions and contexts using the tokenizer\n",
    "inputs = tokenizer(\n",
    "    train_data[2:6][\"question\"],       # List of questions to be tokenized\n",
    "    train_data[2:6][\"context\"],        # List of contexts to be tokenized\n",
    "    max_length=100,                    # Set the maximum combined sequence length\n",
    "    truncation=\"only_second\",          # Truncate context if it exceeds max_length\n",
    "    stride=50,                         # Step size for sliding the tokenized window\n",
    "    return_overflowing_tokens=True,    # Return overflowing tokens\n",
    "    return_offsets_mapping=True,       # Return offsets mapping for the original text\n",
    ")\n",
    "\n",
    "# Calculate the number of tokenized sequences generated\n",
    "num_tokenized_sequences = len(inputs['input_ids'])\n",
    "\n",
    "# Print the number of tokenized sequences and where they come from\n",
    "print(f\"The {num_tokenized_sequences} examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f1bd1-8f0c-472c-baf2-2093fa981e0e",
   "metadata": {},
   "source": [
    "### 4.4 Extracting Start and End Token Positions \n",
    "The code processes tokenized sequences and aligns them with the original answer positions in the context to extract start and end token positions for each answer span. The goal is to identify the token positions corresponding to the start and end of the answer text within the tokenized context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d558c564-9f4f-4173-ae9b-e3de3c31ecd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Positions: [81, 49, 17, 0, 0, 57, 19, 33, 0, 0, 0, 63, 27, 0, 0, 0, 0]\n",
      "End Positions: [83, 51, 19, 0, 0, 63, 25, 39, 0, 0, 0, 64, 28, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get the answers from the original dataset for the processed examples\n",
    "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
    "\n",
    "# Initialize empty lists to store start and end positions\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "# Loop through each offset mapping in the tokenized inputs\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    # Get the index of the example in the original dataset\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    # Retrieve the answer for the example\n",
    "    answer = answers[sample_idx]\n",
    "    # Calculate the start and end character positions of the answer in the context\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    # Get sequence IDs for the tokenized example\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end token positions of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # Check if the answer is fully inside the context\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        # If not, label the start and end positions as (0, 0)\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise, find the start token position within the context\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        # Find the end token position within the context\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "# Print the lists of start and end positions\n",
    "print(\"Start Positions:\", start_positions)\n",
    "print(\"End Positions:\", end_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e933b-e419-4596-abcf-09dd74993cb4",
   "metadata": {},
   "source": [
    "### 4.5 Verifying our Approach\n",
    "This code snippet illustrates how the tokenized answer span is decoded and compared to the original answer text, serving as a visual verification step to ensure that the preprocessing and tokenization were successful in retaining the semantic content of the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49a6901f-3385-456a-bcb2-ec81ff7a8301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: the Main Building, labels give: the main building\n"
     ]
    }
   ],
   "source": [
    "# Initialize index to select an example\n",
    "idx = 0\n",
    "\n",
    "# Obtain the sample index for the current tokenized sequence\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "\n",
    "# Retrieve the answer text for the current example\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "# Obtain the start and end token positions for the answer span\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "\n",
    "# Decode the tokenized answer span using the start and end positions\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "\n",
    "# Print the comparison between the theoretical answer and the labeled answer span\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906716c1-46f4-4df0-8240-e2aa52b5312f",
   "metadata": {},
   "source": [
    "### 4.6 Consolidating The Code\n",
    "This code defines a function called `preprocess_training_examples` that preprocesses training examples for a question answering task using the SQuAD dataset. It tokenizes both questions and contexts using the specified `max_length` and `stride` parameters. It then processes the tokenized inputs to extract answer positions within the context, considering overflow, offset mappings, and sequence IDs. Finally, it adds the start and end positions of the answer spans to the tokenized inputs and returns the processed inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fce911ed-a402-4149-bc24-cfadc4fb8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence length and stride for tokenization\n",
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "# Define a function to preprocess training examples\n",
    "def preprocess_training_examples(examples):\n",
    "    # Strip leading and trailing whitespace from question texts\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    \n",
    "    # Tokenize questions and contexts using the tokenizer\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Extract offset mappings and sample mappings\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # Extract answers from the examples\n",
    "    answers = examples[\"answers\"]\n",
    "    \n",
    "    # Initialize lists to store start and end positions for answer spans\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Iterate over each tokenized example and process answer positions\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    # Add start and end positions to the tokenized inputs\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d616d7f-5bef-4439-92d0-5ea37270451e",
   "metadata": {},
   "source": [
    "### 4.7 Preprocessing the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d685821-8acb-448e-bf86-1a30101d86a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d761f33e21f427795db18c8a4a99f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599 88524\n"
     ]
    }
   ],
   "source": [
    "# Map the preprocess_training_examples function to the training examples\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Get the lengths of the original and processed datasets\n",
    "original_dataset_length = len(raw_datasets[\"train\"])\n",
    "processed_dataset_length = len(train_dataset)\n",
    "\n",
    "# Print the lengths\n",
    "print(original_dataset_length, processed_dataset_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e97de-21cd-4b81-b42e-9c2ed70b7ec4",
   "metadata": {},
   "source": [
    "### 4.8 Preprocessing the Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed88d8e5-3825-4ef8-8732-271508f8ad3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad36c37423744b5b2155ed73bdb357c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570 10784\n"
     ]
    }
   ],
   "source": [
    "# Map the preprocess_training_examples function to the validation examples\n",
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_training_examples,      # Use the same preprocessing function\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "# Get the lengths of the original and processed validation datasets\n",
    "original_validation_length = len(raw_datasets[\"validation\"])\n",
    "processed_validation_length = len(validation_dataset)\n",
    "\n",
    "# Print the lengths\n",
    "print(original_validation_length, processed_validation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc4df1-ece4-4ffb-abb3-2124b98e3dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af19bb90-7c27-4ebe-bbf9-d337625a9e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfcf5e1-6f6d-4081-873f-77f70eec2344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0957b093-0623-413e-848c-23be48623465",
   "metadata": {},
   "source": [
    "## 5 Designing a Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1616da8-c2ce-43e3-ac94-d8def8719738",
   "metadata": {},
   "source": [
    "### 5.1 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67435db5-b627-46b0-abb7-21a55daabf7a",
   "metadata": {},
   "source": [
    "### 5.2 Model Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aaabbf-efdf-41fa-97ed-718ccf740991",
   "metadata": {},
   "source": [
    "### 5.3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b649d1-48a8-46e8-af66-602c0a0a5d27",
   "metadata": {},
   "source": [
    "### 5.4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb71ed-53a2-41eb-935e-bd982804621f",
   "metadata": {},
   "source": [
    "## 6 Implementing Transformer-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374831b6-1776-4fe4-8b9e-82e9025358b1",
   "metadata": {},
   "source": [
    "## 7 Using the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d36a5-9708-400c-8c16-545ae6550877",
   "metadata": {},
   "source": [
    "## 8 Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
